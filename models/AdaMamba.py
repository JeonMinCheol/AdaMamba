import torch
import torch.nn as nn
import numpy as np
import os, sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from layers.AdaMamba_backbone import *
from layers.AdaMamba_adaptive_blocks import AdaptiveNormalizationBlock
from layers.AdaMamba_experts_blocks import ContextEncoder
from utils.metrics import quantile_loss

class Model(nn.Module):
    def __init__(self, configs):
        super().__init__()
        self.configs = configs
        self.pred_len = configs.pred_len
        self.seq_len = configs.seq_len
        self.c_in = configs.enc_in
        self.d_model = configs.d_model
        self.is_training = configs.is_training
        self.lambda_h_loss = configs.lambda_h_loss
        self.lambda_q_loss = configs.lambda_q_loss
        
        self.adaptive_norm_block = AdaptiveNormalizationBlock(configs)
        self.encoder = ContextEncoder(configs)
        self.mean_head = PredictionHead(configs)
        self.trend_gate = nn.Parameter(torch.zeros(1))

    def forward(self, x_enc, batch_y):
        B, L, M = x_enc.shape
        x_enc = x_enc.permute(0, 2, 1).reshape(B * M, L, 1)
        y_true = batch_y[:, -self.pred_len:, :].permute(0, 2, 1).reshape(B * M, self.pred_len, 1)

        normalized_x, means, stdev, trend = self.adaptive_norm_block.normalize(x_enc)
        summary_context = self.encoder(normalized_x)
        mean_pred_norm = self.mean_head(summary_context)

        # ============================================================
        # ğŸ”¥ [ì—…ê·¸ë ˆì´ë“œ] Robust Global Trend Fusion (RGT)
        # ============================================================
        
        # (1) í˜•íƒœ ë³€í™˜: (B*M, L, 1) -> (B, M, L, 1)
        trend_reshaped = trend.reshape(B, M, L, 1)
        
        # (2) 1ì°¨ ë‹¨ìˆœ í‰ê·  (Naive Mean) ê³„ì‚°
        # ì¼ë‹¨ ë‹¤ ì„ì–´ì„œ ëŒ€ëµì ì¸ 'ì¤‘ì‹¬'ì„ ì¡ìŠµë‹ˆë‹¤.
        naive_mean = torch.mean(trend_reshaped, dim=1, keepdim=True) # (B, 1, L, 1)
        
        # (3) "ì´ìƒì¹˜ ì ìˆ˜" ê³„ì‚° (Distance from Mean)
        # ë‚´ íŠ¸ë Œë“œê°€ ì „ì²´ í‰ê· ê³¼ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ê°€? (L1 Distance)
        # ì°¨ì´ê°€ í´ìˆ˜ë¡ ì´ìƒì¹˜(Outlier)ì¼ í™•ë¥ ì´ ë†’ìŒ
        deviation = torch.mean(torch.abs(trend_reshaped - naive_mean), dim=2, keepdim=True) # (B, M, 1, 1)
        
        # (4) ê±°ë¦¬ ì—­ìˆ˜ ê°€ì¤‘ì¹˜ (Softmax)
        # ì „ì²´ íë¦„ê³¼ ë¹„ìŠ·í•œ(ê±°ë¦¬ê°€ ê°€ê¹Œìš´) ì±„ë„ì¼ìˆ˜ë¡ ê°€ì¤‘ì¹˜ë¥¼ ë†’ê²Œ ì¤Œ
        # temperature(tau)ë¥¼ 0.5 ì •ë„ë¡œ ì£¼ì–´ ë³€ë³„ë ¥ ê°•í™”
        weights = torch.softmax(-deviation / 0.5, dim=1) # (B, M, 1, 1)
        
        # (5) Robust Global Trend ìƒì„± (ê°€ì¤‘ í‰ê· )
        global_trend = torch.sum(trend_reshaped * weights, dim=1, keepdim=True)
        
        # (6) ì£¼ì… (ê¸°ì¡´ê³¼ ë™ì¼)
        fused_trend = trend_reshaped + self.trend_gate * global_trend
        trend = fused_trend.reshape(B * M, L, 1)
        
        # ============================================================

        y_true_detrended = y_true - trend[:, -y_true.size(1):, :]
        normalized_y_true = (y_true_detrended - means) / stdev

        # 5. Losses
        huber = F.smooth_l1_loss(mean_pred_norm, normalized_y_true)
        q10 = quantile_loss(mean_pred_norm, normalized_y_true, 0.1)
        q90 = quantile_loss(mean_pred_norm, normalized_y_true, 0.9)
        mean_loss = self.lambda_h_loss * huber + self.lambda_q_loss * (q10 + q90)

        return mean_loss

    def sample(self, x_enc):
        self.eval()
        with torch.no_grad():
            B, L, M = x_enc.shape
            x_enc = x_enc.permute(0, 2, 1).reshape(B * M, L, 1)

            # 1. ì •ê·œí™” ë° ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ê¸°ì¡´ê³¼ ë™ì¼)
            normalized_x, means, stdev, trend = self.adaptive_norm_block.normalize(x_enc)
            summary_context = self.encoder(normalized_x) 

            # ============================================================
            # ğŸ”¥ [ì—…ê·¸ë ˆì´ë“œ] Robust Global Trend Fusion (RGT)
            # ============================================================
            
            # (1) í˜•íƒœ ë³€í™˜: (B*M, L, 1) -> (B, M, L, 1)
            trend_reshaped = trend.reshape(B, M, L, 1)
            
            # (2) 1ì°¨ ë‹¨ìˆœ í‰ê·  (Naive Mean) ê³„ì‚°
            # ì¼ë‹¨ ë‹¤ ì„ì–´ì„œ ëŒ€ëµì ì¸ 'ì¤‘ì‹¬'ì„ ì¡ìŠµë‹ˆë‹¤.
            naive_mean = torch.mean(trend_reshaped, dim=1, keepdim=True) # (B, 1, L, 1)
            
            # (3) "ì´ìƒì¹˜ ì ìˆ˜" ê³„ì‚° (Distance from Mean)
            # ë‚´ íŠ¸ë Œë“œê°€ ì „ì²´ í‰ê· ê³¼ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ê°€? (L1 Distance)
            # ì°¨ì´ê°€ í´ìˆ˜ë¡ ì´ìƒì¹˜(Outlier)ì¼ í™•ë¥ ì´ ë†’ìŒ
            deviation = torch.mean(torch.abs(trend_reshaped - naive_mean), dim=2, keepdim=True) # (B, M, 1, 1)
            
            # (4) ê±°ë¦¬ ì—­ìˆ˜ ê°€ì¤‘ì¹˜ (Softmax)
            # ì „ì²´ íë¦„ê³¼ ë¹„ìŠ·í•œ(ê±°ë¦¬ê°€ ê°€ê¹Œìš´) ì±„ë„ì¼ìˆ˜ë¡ ê°€ì¤‘ì¹˜ë¥¼ ë†’ê²Œ ì¤Œ
            # temperature(tau)ë¥¼ 0.5 ì •ë„ë¡œ ì£¼ì–´ ë³€ë³„ë ¥ ê°•í™”
            weights = torch.softmax(-deviation / 0.5, dim=1) # (B, M, 1, 1)
            
            # (5) Robust Global Trend ìƒì„± (ê°€ì¤‘ í‰ê· )
            global_trend = torch.sum(trend_reshaped * weights, dim=1, keepdim=True)
            
            # (6) ì£¼ì… (ê¸°ì¡´ê³¼ ë™ì¼)
            fused_trend = trend_reshaped + self.trend_gate * global_trend
            trend = fused_trend.reshape(B * M, L, 1)
            
            # ============================================================
            
            # 2. í‰ê·  ì˜ˆì¸¡ (mean_head)
            mean_pred_norm = self.mean_head(summary_context)
            
            # 5. ì „ì²´ ìŠ¤ì¼€ì¼ ë³µì› (De-normalization)
            trend_for_forecast = trend[:, -self.pred_len:, :]
            final_forecast = self.adaptive_norm_block.denormalize(mean_pred_norm, means, stdev, trend_for_forecast).reshape(B, M, self.pred_len).permute(0, 2, 1)
            
            return final_forecast

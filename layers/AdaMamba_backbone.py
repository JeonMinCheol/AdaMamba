import torch
import torch.nn as nn
import torch.nn.functional as F

import math
from .Normalizing_Flows import create_conditional_nsf_flow
    
class PredictionHead(nn.Module):
    def __init__(self, configs):
        super().__init__()
        self.pred_len = configs.pred_len
        self.enc_in = configs.enc_in
        d_model = configs.d_model
        d_inner = configs.d_head
        self.shortcut_dropout = nn.Dropout(configs.head_dropout)
        
        # 1. ë©”ì¸ ê²½ë¡œ (ë¹„ì„ í˜•ì„±ì„ í•™ìŠµ)
        # ì•ˆì •ì ì¸ LayerNormê³¼ GELU ì‚¬ìš©
        self.mlp = nn.Sequential(
            nn.LayerNorm(d_model),
            nn.Linear(d_model, d_inner),
            nn.GELU(),
            nn.Dropout(configs.head_dropout),
            nn.Linear(d_inner, self.pred_len * self.enc_in)
        )

        # 2. ì§€ë¦„ê¸¸ ê²½ë¡œ (Shortcut)
        # ìž…ë ¥ì—ì„œ ì¶œë ¥ìœ¼ë¡œ ë°”ë¡œ ê°€ëŠ” ê³ ì†ë„ë¡œë¥¼ ëš«ì–´ì¤Œ
        # ì´ê²ƒì´ í•™ìŠµ ì•ˆì •ì„±ì„ ë³´ìž¥í•˜ê³  ì„±ëŠ¥ ì €í•˜ë¥¼ ë§‰ìŒ
        self.shortcut = nn.Linear(d_model, self.pred_len * self.enc_in)

    def forward(self, summary_context):
        # ë©”ì¸ ê²½ë¡œì˜ ê²°ê³¼ì™€ ì§€ë¦„ê¸¸ ê²½ë¡œì˜ ê²°ê³¼ë¥¼ ë”í•¨
        # ResNetì˜ ì›ë¦¬ì™€ ìœ ì‚¬í•˜ì—¬ ê·¸ë¼ë””ì–¸íŠ¸ ì†Œì‹¤ì„ ë°©ì§€
        output = self.mlp(summary_context) +  self.shortcut_dropout(self.shortcut(summary_context))
        
        return output.view(-1, self.pred_len, self.enc_in)

# class ProbabilisticResidualModel(nn.Module):
#     def __init__(self, configs):
#         super().__init__()
#         self.c_in = configs.enc_in
#         self.pred_len = configs.pred_len
#         self.d_model = configs.d_model
#         self.window_size = configs.patch_len
#         self.stride = configs.stride
#         self.flow_head = create_conditional_nsf_flow(configs)

#     def forward(self, summary_context, y):
#         # y shape: [B, pred_len, c_in]
#         B = y.size(0)
        
#         # --- ðŸ’¡ [í•µì‹¬] for ë£¨í”„ë¥¼ unfold ì—°ì‚°ìœ¼ë¡œ ëŒ€ì²´ ---
#         # 1. yë¥¼ [B, C, L] í˜•íƒœë¡œ ë³€í™˜
#         y_transposed = y.permute(0, 2, 1)
        
#         # 2. unfoldë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ìœˆë„ìš°ë¥¼ í•œ ë²ˆì— ì¶”ì¶œ
#         # ê²°ê³¼ shape: [B, C, num_windows, window_size]
#         y_windows = y_transposed.unfold(dimension=2, size=self.window_size, step=self.stride)
        
#         # 3. Flowì— ìž…ë ¥í•˜ê¸° ìœ„í•´ shape ìž¬ì •ë ¬ ë° flatten
#         # [B, C, num_windows, window_size] -> [B, num_windows, C, window_size]
#         y_windows = y_windows.permute(0, 2, 1, 3)
#         num_windows = y_windows.shape[1]
        
#         # [B, num_windows, C, window_size] -> [B * num_windows, C * window_size]
#         y_windows_flat = y_windows.reshape(B * num_windows, -1)
        
#         # 4. summary_contextë¥¼ ê° ìœˆë„ìš°ì— ë§žê²Œ í™•ìž¥
#         # [B, d_model] -> [B, num_windows, d_model] -> [B * num_windows, d_model]
#         expanded_context = summary_context.unsqueeze(1).expand(-1, num_windows, -1)
#         expanded_context = expanded_context.reshape(B * num_windows, -1)
        
#         log_prob_windows = self.flow_head.log_prob(y_windows_flat, context=expanded_context)
        
#         nll_loss = -log_prob_windows.mean()
        
#         return nll_loss

#     def sample(self, summary_context, num_samples):
#         self.eval()
#         with torch.no_grad():
#             device = summary_context.device
#             B = summary_context.size(0)
#             S = num_samples
#             W, C, L, stride = self.window_size, self.c_in, self.pred_len, self.stride
            
#             num_windows = (L - W) // stride + 1

#             # 1) ì»¨í…ìŠ¤íŠ¸ í™•ìž¥
#             expanded_context = summary_context.unsqueeze(1).expand(-1, num_windows, -1)
#             expanded_context = expanded_context.reshape(B * num_windows, -1)  # [B*NW, d]

#             # 2) ëª¨ë“  ìœˆë„ìš° ìƒ˜í”Œ ìƒì„± + ë¡œê·¸ ìš°ë„ ë™ì‹œ ê³„ì‚° (ê°œì„ ì  1)
#             # sample_windows_flat: [S, B*NW, W*C]
#             # logp_flat: [S, B*NW]
#             sample_windows_flat, logp_flat = self.flow_head.sample_and_log_prob(
#                 S, context=expanded_context
#             )

#             # 3) ìœˆë„ìš° í…ì„œë¡œ ë³µì›: [S, B, NW, W, C]
#             sample_windows = sample_windows_flat.view(S, B, num_windows, W, C)

#             # 4) ìš°ë„ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¡œê·¸ ìš°ë„ í…ì„œ ë³µì›)
#             logp = logp_flat.view(S, B, num_windows)
#             logp_centered = logp - logp.amax(dim=2, keepdim=True)
#             w_win = torch.softmax(logp_centered, dim=2)  # [S, B, NW]

#             # 5) ê°€ì¤‘ OLA ë²¡í„°í™” (ê°œì„ ì  2: F.fold ì‚¬ìš©)
            
#             # 5a) ê°€ì¤‘ì¹˜ ì¤€ë¹„
#             w_hann = torch.hann_window(W, periodic=False, device=device) # [W]
            
#             # ë¸Œë¡œë“œìºìŠ¤íŒ…ì„ ìœ„í•œ í…ì„œ shape
#             w_time = w_hann.view(1, 1, 1, W, 1)  # [1, 1, 1, W, 1] (Hann)
#             w_prob = w_win.view(S, B, num_windows, 1, 1) # [S, B, NW, 1, 1] (Likelihood)

#             # 5b) ë‘ ê°€ì¤‘ì¹˜ ì ìš©
#             # [S, B, NW, W, C] * [1, 1, 1, W, 1] * [S, B, NW, 1, 1]
#             weighted_samples = sample_windows * w_time * w_prob
            
#             # ì •ê·œí™”ìš© ê°€ì¤‘ì¹˜ í•© (C ì±„ë„ë¡œ expand)
#             # [1, 1, 1, W, 1] * [S, B, NW, 1, 1] -> [S, B, NW, W, 1]
#             combined_weights = (w_time * w_prob).expand(-1, -1, -1, -1, C)

#             # 5c) F.foldë¥¼ ìœ„í•œ í…ì„œ ë³€í˜•
#             # fold ìž…ë ¥ í˜•ì‹: [N, C*K, L_out] = [S*B, (C*W), NW]
#             N = S * B
            
#             # [S, B, NW, W, C] -> [S, B, NW, C, W] -> [S*B, NW, C*W] -> [N, C*W, NW]
#             fold_input = weighted_samples.permute(0, 1, 2, 4, 3).reshape(N, num_windows, C * W).permute(0, 2, 1)
#             fold_weights = combined_weights.permute(0, 1, 2, 4, 3).reshape(N, num_windows, C * W).permute(0, 2, 1)

#             # 5d) F.fold ì‹¤í–‰ (1D ì‹œê³„ì—´ì„ 2D ì´ë¯¸ì§€ì²˜ëŸ¼ ì²˜ë¦¬)
#             output_size = (L, 1)  # (pred_len, 1)
#             kernel_size = (W, 1)  # (window_size, 1)
#             stride_1d = (stride, 1) # (stride, 1)

#             final_samples_flat = F.fold(
#                 fold_input, output_size=output_size, kernel_size=kernel_size, stride=stride_1d
#             )
#             weight_sum_flat = F.fold(
#                 fold_weights, output_size=output_size, kernel_size=kernel_size, stride=stride_1d
#             )
            
#             # 5e) fold ì¶œë ¥ í…ì„œ ë³µì›
#             # fold ì¶œë ¥: [N, C, H_out, W_out] = [S*B, C, L, 1]
#             # [S*B, C, L, 1] -> [S, B, C, L] -> [S, B, L, C] (ìµœì¢… shape)
#             final_samples = final_samples_flat.squeeze(-1).view(S, B, C, L).permute(0, 1, 3, 2)
#             weight_sum = weight_sum_flat.squeeze(-1).view(S, B, C, L).permute(0, 1, 3, 2)

#             # 6) ìµœì¢… ì •ê·œí™”
#             final_samples = final_samples / weight_sum.clamp_min(1e-6)
            
#             return final_samples
        
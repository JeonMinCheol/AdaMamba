import numpy as np
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import matplotlib.ticker as mticker
import csv, os
import seaborn as sns
import torch.distributed as dist

import io
from PIL import Image

from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

from scipy.stats import pearsonr
from sklearn.metrics.pairwise import cosine_similarity
import os
import pandas as pd # pandasê°€ ì—†ë‹¤ë©´ ì¶”ê°€

plt.switch_backend('agg')

class RMSNorm(nn.Module):
    def __init__(self, d_model, eps=1e-8):
        super().__init__()
        self.scale = nn.Parameter(torch.ones(d_model))
        self.eps = eps

    def forward(self, x):
        # Root Mean Square ê³„ì‚°
        rms = x.pow(2).mean(dim=-1, keepdim=True).sqrt()
        return x / (rms + self.eps) * self.scale

def dict_to_string(dict: dict):
    ret = ""
    for k, v in dict.items():
        ret += k + ": " + str(v.item()) +", "
    return ret

def adjust_learning_rate(optimizer, scheduler, epoch, args, printout=True):
    # lr = args.learning_rate * (0.2 ** (epoch // 2))
    if args.lradj == 'type1':
        lr_adjust = {epoch: args.learning_rate * (0.5 ** ((epoch - 1) // 1))}
    elif args.lradj == 'type2':
        lr_adjust = {
            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,
            10: 5e-7, 15: 1e-7, 20: 5e-8
        }
    elif args.lradj == 'type3':
        lr_adjust = {epoch: args.learning_rate if epoch < 3 else args.learning_rate * (0.9 ** ((epoch - 3) // 1))}
    elif args.lradj == 'constant':
        lr_adjust = {epoch: args.learning_rate}
    elif args.lradj == '3':
        lr_adjust = {epoch: args.learning_rate if epoch < 10 else args.learning_rate*0.1}
    elif args.lradj == '4':
        lr_adjust = {epoch: args.learning_rate if epoch < 15 else args.learning_rate*0.1}
    elif args.lradj == '5':
        lr_adjust = {epoch: args.learning_rate if epoch < 25 else args.learning_rate*0.1}
    elif args.lradj == '6':
        lr_adjust = {epoch: args.learning_rate if epoch < 5 else args.learning_rate*0.1}  
    elif args.lradj == 'TST':
        lr_adjust = {epoch: scheduler.get_last_lr()[0]}
    
    if epoch in lr_adjust.keys():
        lr = lr_adjust[epoch]
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr
        
        if printout:
            print('Updating learning rate to {}'.format(lr))

class EarlyStopping:
    def __init__(self, patience=7, verbose=True, delta=0):
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = float("inf")
        self.delta = delta

    def __call__(self, val_loss, test_loss, model, path):
        if not dist.is_initialized():
            # Single GPU fallback
            return self._update(val_loss, model, path)

        # 1) ëª¨ë“  í”„ë¡œì„¸ìŠ¤ì˜ val_loss ëª¨ìœ¼ê¸°
        device = next(model.parameters()).device
        tensor_loss = torch.tensor([val_loss], dtype=torch.float32, device=device)
        all_losses = [torch.zeros_like(tensor_loss) for _ in range(dist.get_world_size())]
        dist.all_gather(all_losses, tensor_loss)
        all_losses = [t.item() for t in all_losses]

        # 2) í˜„ì¬ ìŠ¤í…ì—ì„œ ê°€ì¥ ë‚®ì€ val_loss ê°€ì§„ rank ì°¾ê¸°
        best_rank = min(range(len(all_losses)), key=lambda r: all_losses[r])
        best_val_loss = all_losses[best_rank]

        # 3) ê¸°ì¡´ best_lossë³´ë‹¤ ë” ì¢‹ì•„ì•¼ë§Œ ì €ì¥
        if best_val_loss + self.delta < self.val_loss_min:
            if dist.get_rank() == best_rank:
                self.save_checkpoint(best_val_loss, test_loss, model, path, best_rank)
            self.val_loss_min = best_val_loss
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True

        # 4) early_stop ìƒíƒœ ë™ê¸°í™”
        flag = torch.tensor(1 if self.early_stop else 0, device=device)
        dist.broadcast(flag, src=best_rank)
        self.early_stop = flag.item() == 1

    def save_checkpoint(self, val_loss, test_loss, model, path, rank):
        if self.verbose:
            print(f"[Rank {rank}] Validation loss decreased "
                  f"({self.val_loss_min:.6f} â†’ {val_loss:.6f}). Saving model ...\nTest loss: {test_loss:6f}")
        os.makedirs(path, exist_ok=True)
        state_dict = model.module.state_dict() if hasattr(model, "module") else model.state_dict()
        torch.save(state_dict, os.path.join(path, f"checkpoint.pth"))

class SimpleLogger:
    def __init__(self, log_dir="logs", filename="train_log.csv"):
        os.makedirs(log_dir, exist_ok=True)
        self.csv_path = os.path.join(log_dir, filename)
        # í—¤ë” ì‘ì„±
        if not os.path.exists(self.csv_path):
            with open(self.csv_path, "w", newline="") as f:
                writer = csv.writer(f)
                writer.writerow(["stage","epoch","step",
                                 "det_mse","res_mse","nll",
                                 "grad_norm","lr","time"])

    def log(self, **kwargs):
        # ì½˜ì†” ì¶œë ¥
        msg = " | ".join([f"{k}:{v:.5f}" if isinstance(v,(float,int)) else f"{k}:{v}"
                          for k,v in kwargs.items()])
        print(msg)

        # CSV ê¸°ë¡
        with open(self.csv_path, "a", newline="") as f:
            writer = csv.writer(f)
            writer.writerow([kwargs.get("stage"),
                             kwargs.get("epoch"),
                             kwargs.get("step"),
                             kwargs.get("det_mse", ""),
                             kwargs.get("res_mse", ""),
                             kwargs.get("nll", ""),
                             kwargs.get("grad_norm", ""),
                             kwargs.get("lr", ""),
                             kwargs.get("time","")])

class dotdict(dict):
    """dot.notation access to dictionary attributes"""
    __getattr__ = dict.get
    __setattr__ = dict.__setitem__
    __delattr__ = dict.__delitem__

class StandardScaler():
    def __init__(self, mean, std):
        self.mean = mean
        self.std = std

    def transform(self, data):
        return (data - self.mean) / self.std

    def inverse_transform(self, data):
        return (data * self.std) + self.mean

def visual(true, preds=None, name='./pic/test.pdf',
           title="Forecast vs Ground Truth",
           xlabel="Time", ylabel="Value"):
    
    plt.figure(figsize=(10, 4))                       
    plt.plot(true, label='Ground Truth',
             color="#1e7cc0", linewidth=1.4, alpha=0.9)
    if preds is not None:
        plt.plot(preds, label='Prediction',
                 color="#ff0000", linewidth=1.0, alpha=0.7)

    # plt.title(title, fontsize=14, weight='bold', pad=12)
    plt.xlabel(xlabel, fontsize=12)
    plt.ylabel(ylabel, fontsize=12)

    # ëˆˆê¸ˆ/ê²©ì ê°€ë…ì„±
    plt.grid(True, linestyle='--', linewidth=0.6, alpha=0.7)
    plt.xticks(fontsize=10)
    plt.yticks(fontsize=10)
    plt.gca().xaxis.set_major_locator(mticker.MaxNLocator(integer=True))

    plt.legend(fontsize=11, loc='best', frameon=True)
    plt.tight_layout()
    plt.savefig(name, bbox_inches='tight', dpi=300)
    plt.close()

def test_params_flop(model,x_shape):
    """
    If you want to thest former's flop, you need to give default value to inputs in model.forward(), the following code can only pass one argument to forward()
    """
    model_params = 0
    for parameter in model.parameters():
        model_params += parameter.numel()
        print('INFO: Trainable parameter count: {:.2f}M'.format(model_params / 1000000.0))
    
    # from ptflops import get_model_complexity_info    
    # with torch.cuda.device(0):
    #     macs, params = get_model_complexity_info(model.cuda(), x_shape, as_strings=True, print_per_layer_stat=True)
    #     print('Flops:' + flops)
    #     print('Params:' + params)
    #     print('{:<30}  {:<8}'.format('Computational complexity: ', macs))
    #     print('{:<30}  {:<8}'.format('Number of parameters: ', params))

    # get_model_complexity_info:  ë„¤íŠ¸ì›Œí¬ì˜ ì—°ì‚°ëŸ‰ê³¼ íŒ¨ëŸ¬ë¯¸í„° ìˆ˜ë¥¼ ì¶œë ¥
        # model: nn.Modules í´ë˜ìŠ¤ë¡œ ë§Œë“¤ì–´ì§„ ê°ì²´. ì—°ì‚°ëŸ‰ê³¼ íŒ¨ëŸ¬ë¯¸í„° ìˆ˜ë¥¼ ì¸¡ì •í•  ë„¤íŠ¸ì›Œí¬ì…ë‹ˆë‹¤.
        # input_res: ì…ë ¥ë˜ëŠ” í…ì„œì˜ ëª¨ì–‘ì„ ë‚˜íƒ€ë‚´ëŠ” tuple. ì´ ë•Œ, batch sizeì— í•´ë‹¹í•˜ëŠ” ì°¨ì›ì€ ì œì™¸í•©ë‹ˆë‹¤.
        # print_per_layer_stat: Trueì¼ ì‹œ, Layer ë‹¨ìœ„ë¡œ ì—°ì‚°ëŸ‰ê³¼ íŒ¨ëŸ¬ë¯¸í„° ìˆ˜ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
        # as_strings: Trueì¼ ì‹œ, ì—°ì‚°ëŸ‰ ë° íŒ¨ëŸ¬ë¯¸í„° ìˆ˜ë¥¼ stringìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤.
        # verbose: Trueì¼ ì‹œ, zero-opì— ëŒ€í•œ warningì„ ì¶œë ¥í•©ë‹ˆë‹¤.

def plot_attention_heatmap(attention_weights, layer_num, head_num, title, sample_num=0):
    """
    ì£¼ì–´ì§„ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¡œ íˆíŠ¸ë§µì„ ê·¸ë¦¬ëŠ” í•¨ìˆ˜.

    Args:
        attention_weights (torch.Tensor): ì–´í…ì…˜ ê°€ì¤‘ì¹˜ í…ì„œ. Shape: [B, n_heads, L, L]
        layer_num (int): ì‹œê°í™”í•  ë ˆì´ì–´ ë²ˆí˜¸.
        head_num (int): ì‹œê°í™”í•  í—¤ë“œ ë²ˆí˜¸.
        sample_num (int): ì‹œê°í™”í•  ë°°ì¹˜ì˜ ìƒ˜í”Œ ë²ˆí˜¸.
    """
    # ì‹œê°í™”í•  íŠ¹ì • ìƒ˜í”Œ, íŠ¹ì • í—¤ë“œì˜ ì–´í…ì…˜ ë§µ ì„ íƒ
    # CPUë¡œ ë°ì´í„° ì´ë™ ë° numpy ë°°ì—´ë¡œ ë³€í™˜
    attn_map = attention_weights[sample_num, head_num].detach().cpu().numpy()

    plt.figure(figsize=(10, 8))
    sns.heatmap(attn_map, cmap='viridis') # 'viridis', 'plasma' ë“± ë‹¤ì–‘í•œ ìƒ‰ìƒ ë§µ ì‚¬ìš© ê°€ëŠ¥
    
    plt.title(f'Attention Heatmap (Layer {layer_num+1}, Head {head_num+1})')
    plt.xlabel('Key (Attended Patches)')
    plt.ylabel('Query (Current Patches)')
    plt.savefig(title, bbox_inches='tight', dpi=300)

def get_heatmap_image_tensor(attention_weights, sample_num=0, head_num=0):
    """
    ì–´í…ì…˜ ê°€ì¤‘ì¹˜ë¡œ íˆíŠ¸ë§µì„ ê·¸ë ¤ PyTorch ì´ë¯¸ì§€ í…ì„œë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.
    
    Args:
        attention_weights (torch.Tensor): ì–´í…ì…˜ ê°€ì¤‘ì¹˜. Shape: [B, n_heads, L, L]
        sample_num (int): ì‹œê°í™”í•  ë°°ì¹˜ì˜ ìƒ˜í”Œ ë²ˆí˜¸.
        head_num (int): ì‹œê°í™”í•  í—¤ë“œ ë²ˆí˜¸.

    Returns:
        torch.Tensor: ì´ë¯¸ì§€ í…ì„œ. Shape: [C, H, W]
    """
    # íŠ¹ì • ì–´í…ì…˜ ë§µ ì„ íƒ
    attn_map = attention_weights[sample_num, head_num].detach().cpu().numpy()

    # Matplotlib Figure ìƒì„±
    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(attn_map, cmap='viridis', ax=ax, cbar=False) # cbarëŠ” ìƒëµ ê°€ëŠ¥
    ax.set_title(f'Attention Heatmap (Head {head_num+1})')
    ax.set_xlabel('Key Patches')
    ax.set_ylabel('Query Patches')
    fig.tight_layout()

    # ğŸ’¡ Figureë¥¼ ë©”ëª¨ë¦¬ ë²„í¼ì— PNG ì´ë¯¸ì§€ë¡œ ì €ì¥
    buf = io.BytesIO()
    fig.savefig(buf, format='png')
    buf.seek(0)

    # ğŸ’¡ ë²„í¼ì˜ ì´ë¯¸ì§€ë¥¼ PILë¡œ ì—´ê³  Numpy ë°°ì—´ë¡œ ë³€í™˜
    img = Image.open(buf)
    img_array = np.array(img.convert('RGB'))

    # ğŸ’¡ Numpy ë°°ì—´ì„ PyTorch í…ì„œë¡œ ë³€í™˜í•˜ê³ , ì±„ë„ ìˆœì„œ ë³€ê²½ [H, W, C] -> [C, H, W]
    img_tensor = torch.from_numpy(img_array).permute(2, 0, 1)

    # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ë¥¼ ìœ„í•´ figureì™€ bufferë¥¼ ë‹«ìŒ
    plt.close(fig)
    buf.close()

    return img_tensor

def log_layer_stats(writer, model, step, optimizer, prefix="stats"):
    for name, p in model.named_parameters():
        if not p.requires_grad: 
            continue
        w = p.data
        g = p.grad
        writer.add_scalar(f"weight_{prefix}/{name}_w_mean", w.mean().item(), step)
        writer.add_scalar(f"weight_{prefix}/{name}_w_std", w.std().item(), step)
        if g is None:
            writer.add_scalar(f"grad_{prefix}/{name}_norm", 0.0, step)
        else:
            writer.add_scalar(f"grad_{prefix}/{name}_norm", g.norm().item(), step)

        if p.grad is not None:
            update = -optimizer.param_groups[0]['lr'] * p.grad
            rel_update = (update.abs().mean() / (p.data.abs().mean() + 1e-8)).item()
            writer.add_scalar(f"update_ratio/{name}", rel_update, step)

def visual_trend(true_trend, pred_trend, raw_data=None, name='./trend_test.pdf'):
    """
    true_trend: (L, ) or (L, 1) - ì‹¤ì œ íŠ¸ë Œë“œ (Moving Average ë“±)
    pred_trend: (L, ) or (L, 1) - ëª¨ë¸ì´ ì¶”ì¶œí•œ íŠ¸ë Œë“œ
    raw_data: (L, ) or (L, 1) - ì›ë³¸ ì‹œê³„ì—´ ë°ì´í„° (ì„ íƒ ì‚¬í•­)
    """
    plt.figure(figsize=(10, 4))
    if raw_data is not None:
        plt.plot(raw_data, label='Raw Data', color='gray', linewidth=0.8, alpha=0.3)
    
    plt.plot(true_trend, label='Actual Trend (MA)', color='blue', linewidth=1.5, alpha=0.8)
    plt.plot(pred_trend, label='Model Extracted Trend', color='red', linewidth=1.5, linestyle='--', alpha=0.8)
    
    plt.title("Trend Comparison", fontsize=12)
    plt.xlabel("Time", fontsize=10)
    plt.ylabel("Value", fontsize=10)
    plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)
    plt.legend(fontsize=10)
    plt.tight_layout()
    plt.savefig(name, bbox_inches='tight', dpi=300)
    plt.close()

# [Added] íŠ¸ë Œë“œ ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜
def calc_trend_similarity(true_trend, pred_trend):
    """
    MSEì™€ Cosine Similarityë¥¼ ê³„ì‚°í•˜ì—¬ ë°˜í™˜
    """
    # MSE
    mse = np.mean((true_trend - pred_trend)**2)
    
    # Cosine Similarity
    t = true_trend.flatten()
    p = pred_trend.flatten()
    
    norm_t = np.linalg.norm(t)
    norm_p = np.linalg.norm(p)
    
    if norm_t == 0 or norm_p == 0:
        cos_sim = 0.0
    else:
        cos_sim = np.dot(t, p) / (norm_t * norm_p)
        
    return mse, cos_sim

# [Added] Moving Average ê³„ì‚° í•¨ìˆ˜ (Ground Truth Trend ìƒì„±ìš©)
def moving_average(data, kernel_size=25):
    """
    data: (B, L, D) Tensor
    kernel_size: int
    return: (B, L, D) Tensor (Same length with padding)
    """
    # (B, D, L) í˜•íƒœë¡œ ë³€ê²½í•˜ì—¬ AvgPool1d ì ìš©
    x = data.permute(0, 2, 1)
    
    # Same paddingì„ ìœ„í•´ ì•ë’¤ë¡œ íŒ¨ë”©
    pad_l = kernel_size // 2
    pad_r = kernel_size - 1 - pad_l
    
    # ReplicationPad1d ë“±ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆì§€ë§Œ, ì—¬ê¸°ì„  ê°„ë‹¨íˆ zero padding í˜¹ì€ replication ì‚¬ìš©
    # íŠ¸ë Œë“œì´ë¯€ë¡œ ê²½ê³„ ë¶€ë¶„ ì²˜ë¦¬ê°€ ì¤‘ìš”í•˜ì§€ë§Œ ê°„ë‹¨í•œ ë¹„êµë¥¼ ìœ„í•´ ReplicationPad ì‚¬ìš©
    x_padded = torch.nn.functional.pad(x, (pad_l, pad_r), mode='replicate')
    
    avg = torch.nn.functional.avg_pool1d(x_padded, kernel_size=kernel_size, stride=1)
    
    # ì›ë˜ shape (B, L, D)ë¡œ ë³µêµ¬
    return avg.permute(0, 2, 1)